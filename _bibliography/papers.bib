---
---
@online{BergmannHerzogJasa:2024:1,
  bibtex_show = {true},
  author = {Bergmann, Ronny and Herzog, Roland and Jasa, Hajg},
  abstract = {We introduce the convex bundle method to solve convex, non-smooth
              optimization problems on Riemannian manifolds of bounded sectional
              curvature. Each step of our method is based on a model that
              involves the convex hull of previously collected subgradients,
              parallelly transported into the current serious iterate. This
              approach generalizes the dual form of classical bundle subproblems
              in Euclidean space. We prove that, under mild conditions, the
              convex bundle method converges to a minimizer. Several numerical
              examples implemented using Manopt.jl illustrate the performance of
              the proposed method and compare it to the subgradient method, the
              cyclic proximal point algorithm, as well as the proximal bundle
              method.},
  month = {feb},
  year = {2024},
  arxiv = {2402.13670},
  title = {The Riemannian Convex Bundle Method},
  preview = {hn-wave-noise.png},
  abbr = {arXiv},
  google_scholar_id = {u5HHmVD_uO8C},
}

@online{AthreyaBergmannJasaKummerleLubberts:2025:1,
  bibtex_show = {true},
  author = {Jasa, Hajg and Bergmann, Ronny and K\"ummerle, Christian and Athreya
            , Avanti and Lubberts, Zachary},
  abstract = {Meaningful comparison between sets of observations often
              necessitates alignment or registration between them, and the
              resulting optimization problems range in complexity from those
              admitting simple closed-form solutions to those requiring advanced
              and novel techniques. We compare different Procrustes problems in
              which we align two sets of points after various perturbations by
              minimizing the norm of the difference between one matrix and an
              orthogonal transformation of the other. The minimization problem
              depends significantly on the choice of matrix norm; we highlight
              recent developments in nonsmooth Riemannian optimization and
              characterize which choices of norm work best for each perturbation.
              We show that in several applications, from low-dimensional
              alignments to hypothesis testing for random networks, when
              Procrustes alignment with the spectral or robust norm is the
              appropriate choice, it is often feasible to replace the
              computationally more expensive spectral and robust minimizers with
              their closed-form Frobenius-norm counterpart. Our work reinforces
              the synergy between optimization, geometry, and statistics.},
  title = {Procrustes Problems on Random Matrices},
  month = {oct},
  year = {2025},
  arxiv = {2510.05182},
  preview = {procrustes.png},
  abbr = {arXiv},
  google_scholar_id = {9yKSN-GCB0IC},
}

@software{BergmannJasa:2025:17277311,
  bibtex_show = {true},
  author = {Bergmann, Ronny and Jasa, Hajg},
  title = {ManoptExamples.jl},
  month = {oct},
  year = {2025},
  publisher = {Zenodo},
  version = {v0.1.16},
  doi = {10.5281/zenodo.17277311},
  abbr = {Zenodo},
}

@online{BergmansnJasaJohnPfeffer:2025:2,
  bibtex_show = {true},
  author = {Bergmann, Ronny and Jasa, Hajg and John, Paula and Pfeffer, Max},
  arxiv = {2507.16055},
  title = {The Intrinsic Riemannian Proximal Gradient Method for Convex
           Optimization},
  abstract = {We consider a class of (possibly strongly) geodesically convex
              optimization problems on Hadamard manifolds, where the objective
              function splits into the sum of a smooth and a possibly nonsmooth
              function. We introduce an intrinsic convex Riemannian proximal
              gradient (CRPG) method that employs the manifold proximal map for
              the nonsmooth step, without operating in the embedding or tangent
              space. A sublinear convergence rate for convex problems and a
              linear convergence rate for strongly convex problems is established
              , and we derive fundamental proximal gradient inequalities that
              generalize the Euclidean case. Our numerical experiments on
              hyperbolic spaces and manifolds of symmetric positive definite
              matrices demonstrate substantial computational advantages over
              existing methods.},
  month = {jul},
  year = {2025},
  preview = {crpg.png},
  abbr = {arXiv},
  google_scholar_id = {d1gkVwhDpl0C},
}

@online{BergmannJasaJohnPfeffer:2025:1,
  bibtex_show = {true},
  author = {Bergmann, Ronny and Jasa, Hajg and John, Paula and Pfeffer, Max},
  arxiv = {2506.09775},
  title = {The Intrinsic Riemannian Proximal Gradient Method for Nonconvex
           Optimization},
  abstract = {We consider the proximal gradient method on Riemannian manifolds
              for functions that are possibly not geodesically convex. Starting
              from the forward-backward-splitting, we define an intrinsic variant
              of the proximal gradient method that uses proximal maps defined on
              the manifold and therefore does not require or work in the
              embedding. We investigate its convergence properties and illustrate
              its numerical performance, particularly for nonconvex or
              nonembedded problems that are hence out of reach for other methods.
              },
  month = {jun},
  year = {2025},
  preview = {ncrpg.png},
  abbr = {arXiv},
  google_scholar_id = {u-x6o8ySG0sC},
}


